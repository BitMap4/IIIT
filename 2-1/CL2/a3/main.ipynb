{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LanguageClassifier import LanguageClassifier\n",
    "from re                 import sub\n",
    "from read_csv           import read_train, read_test\n",
    "from sklearn.metrics    import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define structures for storing classifiers and predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc:            str                           = \"polish_czech\"\n",
    "sp:            str                           = \"spanish_portugese\"\n",
    "classifiers:   dict[str, LanguageClassifier] = dict()\n",
    "predictions_1: dict[str, list[str]]          = dict()\n",
    "predictions_2: dict[str, list[str]]          = dict()\n",
    "predictions_3: dict[str, list[str]]          = dict()\n",
    "predictions_4: dict[str, list[str]]          = dict()\n",
    "actual:        dict[str, list[str]]          = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"train\", \"test\"]:\n",
    "    with open(f\"Language_datasets/{sp}_{dataset}.csv\", \"r\") as f:\n",
    "        lines: str = f.read()\n",
    "        lines      = sub(r\"\\n(\\d+)[,\\t](?!spa)(?!por).*\", r\"\",           lines)\n",
    "        lines      = sub(r\"\\n(\\d+)[^,](spa|por)[^,](.*)\", r\"\\n\\1,\\2,\\3\", lines)\n",
    "    with open(f\"Language_datasets/{sp}_{dataset}.csv\", \"w\") as f:\n",
    "        f.write(lines)\n",
    "for dataset in [\"train\", \"test\"]:\n",
    "    with open(f\"Language_datasets/{pc}_{dataset}.csv\", \"r\") as f:\n",
    "        lines: str = f.read()\n",
    "        lines      = sub(r\"\\n(\\d+)[,\\t](?!pol)(?!ces).*\", r\"\",           lines)\n",
    "        lines      = sub(r\"\\n(\\d+)[^,](pol|ces)[^,](.*)\", r\"\\n\\1,\\2,\\3\", lines)\n",
    "    with open(f\"Language_datasets/{pc}_{dataset}.csv\", \"w\") as f:\n",
    "        f.write(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifiers and then use them to predict the test data\n",
    "Takes about 8 minutes total, for both language pairs on my machine (11th gen i5, 8 cores, 2.4GHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for polish_czech\n",
      "\tMaking classifier\n",
      "\tSetting up languages\n",
      "Making predictions\n",
      "Starting training for spanish_portugese\n",
      "\tMaking classifier\n",
      "\tSetting up languages\n",
      "Making predictions\n"
     ]
    }
   ],
   "source": [
    "for langs in [pc, sp]:\n",
    "    print(f\"Starting training for {langs}\")\n",
    "    train_set: list[str]    = read_train(f\"Language_datasets/{langs}_train.csv\")\n",
    "    classifiers[langs]      = LanguageClassifier(train_set)\n",
    "    test_set:  list[str]\n",
    "    test_set, actual[langs] = read_test(f\"Language_datasets/{langs}_test.csv\")\n",
    "    print(\"Making predictions\")\n",
    "    predictions_1[langs]    = [langs.split('_')[classifiers[langs].classify(text, ignore=[2, 3, 4])] for text in test_set]\n",
    "    predictions_2[langs]    = [langs.split('_')[classifiers[langs].classify(text, ignore=[1, 3, 4])] for text in test_set]\n",
    "    predictions_3[langs]    = [langs.split('_')[classifiers[langs].classify(text, ignore=[1, 2, 4])] for text in test_set]\n",
    "    predictions_4[langs]    = [langs.split('_')[classifiers[langs].classify(text, ignore=[1, 2, 3])] for text in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Â¿eres ciud'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for langs in [pc, sp]:\n",
    "    ngrams: list[dict[str, list[int]]] = [\n",
    "        {key: [0]*len(classifiers[langs].languages) for key in classifiers[langs].unigrams},\n",
    "        {key: [0]*len(classifiers[langs].languages) for key in classifiers[langs].bigrams},\n",
    "        {key: [0]*len(classifiers[langs].languages) for key in classifiers[langs].trigrams},\n",
    "        {key: [0]*len(classifiers[langs].languages) for key in classifiers[langs].quadgrams}\n",
    "    ]\n",
    "\n",
    "    with open(f\"output/Q1/unigrams_{langs}.csv\", \"w\") as f:\n",
    "        for i, lang in enumerate(classifiers[langs].languages):\n",
    "            for unigram in lang.unigrams:\n",
    "                ngrams[0][unigram][i] = lang.unigrams[unigram]\n",
    "        f.write(f\"Unigram,{','.join(langs.split('_'))}\\n\")\n",
    "        for unigram in ngrams[0]:\n",
    "            f.write(f\"{unigram},{','.join(map(str, ngrams[0][unigram]))}\\n\")\n",
    "\n",
    "    with open(f\"output/Q1/bigrams_{langs}.csv\", \"w\") as f:\n",
    "        for i, lang in enumerate(classifiers[langs].languages):\n",
    "            for bigram in lang.bigrams:\n",
    "                ngrams[1][bigram][i] = lang.bigrams[bigram]\n",
    "        f.write(f\"Bigram,{','.join(langs.split('_'))}\\n\")\n",
    "        for bigram in ngrams[1]:\n",
    "            f.write(f\"{bigram},{','.join(map(str, ngrams[1][bigram]))}\\n\")\n",
    "\n",
    "    with open(f\"output/Q1/trigrams_{langs}.csv\", \"w\") as f:\n",
    "        for i, lang in enumerate(classifiers[langs].languages):\n",
    "            for trigram in lang.trigrams:\n",
    "                ngrams[2][trigram][i] = lang.trigrams[trigram]\n",
    "        f.write(f\"Trigram,{','.join(langs.split('_'))}\\n\")\n",
    "        for trigram in ngrams[2]:\n",
    "            f.write(f\"{trigram},{','.join(map(str, ngrams[2][trigram]))}\\n\")\n",
    "\n",
    "    with open(f\"output/Q1/quadgrams_{langs}.csv\", \"w\") as f:\n",
    "        for i, lang in enumerate(classifiers[langs].languages):\n",
    "            for quadgram in lang.quadgrams:\n",
    "                ngrams[3][quadgram][i] = lang.quadgrams[quadgram]\n",
    "        f.write(f\"Quadgram,{','.join(langs.split('_'))}\\n\")\n",
    "        for quadgram in ngrams[3]:\n",
    "            f.write(f\"{quadgram},{','.join(map(str, ngrams[3][quadgram]))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision and recall for polish_czech\n",
      "| ngrams    | Precision          | Recall            |\n",
      "|-----------|--------------------|-------------------|\n",
      "| Unigrams  | 0.9954357721590916 | 0.995428476435874 |\n",
      "| Bigrams   | 0.9980883061493411 | 0.9980882719640928 |\n",
      "| Trigrams  | 0.9988918539039896 | 0.9988917518632422 |\n",
      "| Quadgrams | 0.9883530343793918 | 0.9883356883606239 |\n",
      "\n",
      "Precision and recall for spanish_portugese\n",
      "| ngrams    | Precision          | Recall            |\n",
      "|-----------|--------------------|-------------------|\n",
      "| Unigrams  | 0.96665866723783 | 0.9666193828815346 |\n",
      "| Bigrams   | 0.990241912735074 | 0.9902395856378756 |\n",
      "| Trigrams  | 0.9952695962016461 | 0.9952694525058237 |\n",
      "| Quadgrams | 0.8252179436028418 | 0.8222173058653583 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for langs in [pc, sp]:\n",
    "    print(f\"Precision and recall for {langs}\")\n",
    "    print(f\"| ngrams    | Precision          | Recall            |\")\n",
    "    print(f\"|-----------|--------------------|-------------------|\")\n",
    "    print(f\"| Unigrams  | {precision_score(actual[langs], predictions_1[langs], average='weighted', zero_division=0)} | {recall_score(actual[langs], predictions_1[langs], average='weighted', zero_division=0)} |\")\n",
    "    print(f\"| Bigrams   | {precision_score(actual[langs], predictions_2[langs], average='weighted', zero_division=0)} | {recall_score(actual[langs], predictions_2[langs], average='weighted', zero_division=0)} |\")\n",
    "    print(f\"| Trigrams  | {precision_score(actual[langs], predictions_3[langs], average='weighted', zero_division=0)} | {recall_score(actual[langs], predictions_3[langs], average='weighted', zero_division=0)} |\")\n",
    "    print(f\"| Quadgrams | {precision_score(actual[langs], predictions_4[langs], average='weighted', zero_division=0)} | {recall_score(actual[langs], predictions_4[langs], average='weighted', zero_division=0)} |\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for langs in [pc, sp]:\n",
    "    with open(f\"output/Q2/{langs}_actual.txt\", 'w') as f:\n",
    "        for i in range(len(actual[langs])):\n",
    "            f.write(f\"{actual[langs][i]}\\n\")\n",
    "    with open(f\"output/Q2/{langs}_pred1.txt\", 'w') as f:\n",
    "        for i in range(len(predictions_1[langs])):\n",
    "            f.write(f\"{predictions_1[langs][i]}\\n\")\n",
    "    with open(f\"output/Q2/{langs}_pred2.txt\", 'w') as f:\n",
    "        for i in range(len(predictions_2[langs])):\n",
    "            f.write(f\"{predictions_2[langs][i]}\\n\")\n",
    "    with open(f\"output/Q2/{langs}_pred3.txt\", 'w') as f:\n",
    "        for i in range(len(predictions_3[langs])):\n",
    "            f.write(f\"{predictions_3[langs][i]}\\n\")\n",
    "    with open(f\"output/Q2/{langs}_pred4.txt\", 'w') as f:\n",
    "        for i in range(len(predictions_4[langs])):\n",
    "            f.write(f\"{predictions_4[langs][i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
